{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9fc84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "     |████████████████████████████████| 1.5 MB 4.4 MB/s            \n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.1.18-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "     |████████████████████████████████| 748 kB 60.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (1.1.0)\n",
      "Collecting click\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 8.1 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.6/site-packages (from click->nltk) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.7.0)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.0.3 nltk-3.6.7 regex-2022.1.18\n",
      "Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.6/site-packages (1.19.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-autotime\n",
      "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: ipython in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython-autotime) (7.16.1)\n",
      "Requirement already satisfied: decorator in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (0.17.2)\n",
      "Requirement already satisfied: backcall in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (58.0.4)\n",
      "Requirement already satisfied: pexpect in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (4.3.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (3.0.24)\n",
      "Requirement already satisfied: pickleshare in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: pygments in /srv/conda/envs/notebook/lib/python3.6/site-packages (from ipython->ipython-autotime) (2.11.2)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from jedi>=0.10->ipython->ipython-autotime) (0.7.1)\n",
      "Requirement already satisfied: wcwidth in /srv/conda/envs/notebook/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.5)\n",
      "Requirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.6/site-packages (from traitlets>=4.2->ipython->ipython-autotime) (1.16.0)\n",
      "Requirement already satisfied: ipython-genutils in /srv/conda/envs/notebook/lib/python3.6/site-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
      "Installing collected packages: ipython-autotime\n",
      "Successfully installed ipython-autotime-0.3.1\n",
      "time: 670 µs (started: 2022-02-01 08:36:45 +00:00)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "!pip install ipython-autotime\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11a27ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 328 ms (started: 2022-02-01 08:37:35 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a437b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 606 µs (started: 2022-02-01 09:19:19 +00:00)\n"
     ]
    }
   ],
   "source": [
    "title = \"20_newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aaeeacef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.4 ms (started: 2022-02-01 09:20:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/comp.graphics'):\n",
    "    for i in filenames:\n",
    "        paths.append(str(dirpath)+str(\"/\")+i)\n",
    "for (dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/rec.motorcycles)'):\n",
    "    for i in filenames:\n",
    "        paths.append(str(dirpath)+str(\"/\")+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78f4e30a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c2eac093d8c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.1 ms (started: 2022-02-01 09:20:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ffea428",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3f746008e132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.1 ms (started: 2022-02-01 09:19:57 +00:00)\n"
     ]
    }
   ],
   "source": [
    "path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da97806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 839 µs (started: 2022-02-01 08:42:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def remove_header(data):\n",
    "    try:\n",
    "        ind = data.index('\\n\\n')\n",
    "        data = data[ind:]\n",
    "    except:\n",
    "        print(\"No Header\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ce44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb0ea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 906 µs (started: 2022-02-01 08:42:43 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d80128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 957 µs (started: 2022-02-01 08:42:55 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043b91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 907 µs (started: 2022-02-01 08:43:07 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b856e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 628 µs (started: 2022-02-01 08:43:18 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b2b09fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.23 ms (started: 2022-02-01 08:43:35 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def remove_single_characters(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f66dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.89 ms (started: 2022-02-01 08:43:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def convert_numbers(data):\n",
    "    data = np.char.replace(data, \"0\", \" zero \")\n",
    "    data = np.char.replace(data, \"1\", \" one \")\n",
    "    data = np.char.replace(data, \"2\", \" two \")\n",
    "    data = np.char.replace(data, \"3\", \" three \")\n",
    "    data = np.char.replace(data, \"4\", \" four \")\n",
    "    data = np.char.replace(data, \"5\", \" five \")\n",
    "    data = np.char.replace(data, \"6\", \" six \")\n",
    "    data = np.char.replace(data, \"7\", \" seven \")\n",
    "    data = np.char.replace(data, \"8\", \" eight \")\n",
    "    data = np.char.replace(data, \"9\", \" nine \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b35863f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 869 µs (started: 2022-02-01 08:44:00 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f698c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 800 µs (started: 2022-02-01 08:44:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data, query):\n",
    "    if not query:\n",
    "        data = remove_header(data)        \n",
    "    data = convert_lower_case(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_single_characters(data)\n",
    "    data = stemming(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f8c175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.33 ms (started: 2022-02-01 09:18:35 +00:00)\n"
     ]
    }
   ],
   "source": [
    "postings = pd.DataFrame()\n",
    "frequency = pd.DataFrame()\n",
    "doc = 0\n",
    "\n",
    "for path in paths:\n",
    "    file = open(path, 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    preprocessed_text = preprocess(text, False)\n",
    "    if doc%100 == 0:\n",
    "        print(doc)\n",
    "\n",
    "    tokens = word_tokenize(str(preprocessed_text))\n",
    "    \n",
    "    pos = 0\n",
    "    for token in tokens:\n",
    "        if token in postings:\n",
    "            p = postings[token][0]            \n",
    "\n",
    "            k = [a[0] for a in p]\n",
    "            if doc in k:\n",
    "                for a in p:\n",
    "                    if a[0] == doc:\n",
    "                        a[1].add(pos)\n",
    "            else:\n",
    "                p.append([doc,{pos}])\n",
    "                frequency[token][0] += 1\n",
    "        else:\n",
    "            postings.insert(value=[[[doc, {pos}]]], loc=0, column=token)\n",
    "            frequency.insert(value=[1], loc=0, column=token)\n",
    "\n",
    "        pos += 1\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdd3a1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.2 ms (started: 2022-02-01 08:45:06 +00:00)\n"
     ]
    }
   ],
   "source": [
    "postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fda231c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.52 ms (started: 2022-02-01 08:45:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9269a7f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '20_newsgroups/comp.graphics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3f9ba1977ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Open files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"20_newsgroups/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# For every file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20_newsgroups/comp.graphics'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.1 ms (started: 2022-02-01 09:10:47 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from natsort import natsorted\n",
    "import string\n",
    " \n",
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding =\"ascii\", errors =\"surrogateescape\") as f:\n",
    "        stuff = f.read()\n",
    " \n",
    "    f.close()\n",
    "     \n",
    "    # Remove header and footer.\n",
    "    stuff = remove_header_footer(stuff)\n",
    "     \n",
    "    return stuff\n",
    " \n",
    "def remove_header_footer(final_string):\n",
    "    new_final_string = \"\"\n",
    "    tokens = final_string.split('\\n\\n')\n",
    " \n",
    "    # Remove tokens[0] and tokens[-1]\n",
    "    for token in tokens[1:-1]:\n",
    "        new_final_string += token+\" \"\n",
    "    return new_final_string\n",
    " \n",
    "def preprocessing(final_string):\n",
    "        # Tokenize.\n",
    "    tokenizer = TweetTokenizer()\n",
    "    token_list = tokenizer.tokenize(final_string)\n",
    " \n",
    "    # Remove punctuations.\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    token_list = [word.translate(table) for word in token_list]\n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\")\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in token_list]\n",
    "    token_list = [str for str in stripped_words if str]\n",
    " \n",
    "    # Change to lowercase.\n",
    "    token_list =[word.lower() for word in token_list]\n",
    "    return token_list\n",
    " \n",
    "# In this example, we create the positional index for only 1 folder.\n",
    "folder_names = [\"comp.graphics\"]\n",
    " \n",
    "# Initialize the stemmer.\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "# Initialize the file no.\n",
    "fileno = 0\n",
    " \n",
    "# Initialize the dictionary.\n",
    "pos_index = {}\n",
    " \n",
    "# Initialize the file mapping (fileno -> file name).\n",
    "file_map = {}\n",
    " \n",
    "for folder_name in folder_names:\n",
    " \n",
    "    # Open files.\n",
    "    file_names = natsorted(os.listdir(\"20_newsgroups/\" + folder_name))\n",
    " \n",
    "    # For every file.\n",
    "    for file_name in file_names:\n",
    " \n",
    "        # Read file contents.\n",
    "        stuff = read_file(\"20_newsgroups/\" + folder_name + \"/\" + file_name)\n",
    "         \n",
    "        # This is the list of words in order of the text.\n",
    "        # We need to preserve the order because we require positions.\n",
    "        # 'preprocessing' function does some basic punctuation removal,\n",
    "        # stopword removal etc.\n",
    "        final_token_list = preprocessing(stuff)\n",
    " \n",
    "        # For position and term in the tokens.\n",
    "        for pos, term in enumerate(final_token_list):\n",
    "             \n",
    "                    # First stem the term.\n",
    "                    term = stemmer.stem(term)\n",
    " \n",
    "                    # If term already exists in the positional index dictionary.\n",
    "                    if term in pos_index:\n",
    "                         \n",
    "                        # Increment total freq by 1.\n",
    "                        pos_index[term][0] = pos_index[term][0] + 1\n",
    "                         \n",
    "                        # Check if the term has existed in that DocID before.\n",
    "                        if fileno in pos_index[term][1]:\n",
    "                            pos_index[term][1][fileno].append(pos)\n",
    "                             \n",
    "                        else:\n",
    "                            pos_index[term][1][fileno] = [pos]\n",
    " \n",
    "                    # If term does not exist in the positional index dictionary\n",
    "                    # (first encounter).\n",
    "                    else:\n",
    "                         \n",
    "                        # Initialize the list.\n",
    "                        pos_index[term] = []\n",
    "                        # The total frequency is 1.\n",
    "                        pos_index[term].append(1)\n",
    "                        # The postings list is initially empty.\n",
    "                        pos_index[term].append({})     \n",
    "                        # Add doc ID to postings list.\n",
    "                        pos_index[term][1][fileno] = [pos]\n",
    " \n",
    "        # Map the file no. to the file name.\n",
    "        file_map[fileno] = \"20_newsgroups/\" + folder_name + \"/\" + file_name\n",
    " \n",
    "        # Increment the file no. counter for document ID mapping             \n",
    "        fileno += 1\n",
    " \n",
    "# Sample positional index to test the code.\n",
    "sample_pos_idx = pos_index[\"andrew\"]\n",
    "print(\"Positional Index\")\n",
    "print(sample_pos_idx)\n",
    " \n",
    "file_list = sample_pos_idx[1]\n",
    "print(\"Filename, [Positions]\")\n",
    "for fileno, positions in file_list.items():\n",
    "    print(file_map[fileno], positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e532bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natsort\n",
      "  Downloading natsort-8.1.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: natsort\n",
      "Successfully installed natsort-8.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "time: 2.62 s (started: 2022-02-01 08:48:14 +00:00)\n"
     ]
    }
   ],
   "source": [
    "pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fd0c31c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '20_newsgroups/comp.graphics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3f9ba1977ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Open files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"20_newsgroups/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# For every file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20_newsgroups/comp.graphics'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 91.3 ms (started: 2022-02-01 09:00:41 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from natsort import natsorted\n",
    "import string\n",
    " \n",
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding =\"ascii\", errors =\"surrogateescape\") as f:\n",
    "        stuff = f.read()\n",
    " \n",
    "    f.close()\n",
    "     \n",
    "    # Remove header and footer.\n",
    "    stuff = remove_header_footer(stuff)\n",
    "     \n",
    "    return stuff\n",
    " \n",
    "def remove_header_footer(final_string):\n",
    "    new_final_string = \"\"\n",
    "    tokens = final_string.split('\\n\\n')\n",
    " \n",
    "    # Remove tokens[0] and tokens[-1]\n",
    "    for token in tokens[1:-1]:\n",
    "        new_final_string += token+\" \"\n",
    "    return new_final_string\n",
    " \n",
    "def preprocessing(final_string):\n",
    "        # Tokenize.\n",
    "    tokenizer = TweetTokenizer()\n",
    "    token_list = tokenizer.tokenize(final_string)\n",
    " \n",
    "    # Remove punctuations.\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    token_list = [word.translate(table) for word in token_list]\n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\")\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in token_list]\n",
    "    token_list = [str for str in stripped_words if str]\n",
    " \n",
    "    # Change to lowercase.\n",
    "    token_list =[word.lower() for word in token_list]\n",
    "    return token_list\n",
    " \n",
    "# In this example, we create the positional index for only 1 folder.\n",
    "folder_names = [\"comp.graphics\"]\n",
    " \n",
    "# Initialize the stemmer.\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "# Initialize the file no.\n",
    "fileno = 0\n",
    " \n",
    "# Initialize the dictionary.\n",
    "pos_index = {}\n",
    " \n",
    "# Initialize the file mapping (fileno -> file name).\n",
    "file_map = {}\n",
    " \n",
    "for folder_name in folder_names:\n",
    " \n",
    "    # Open files.\n",
    "    file_names = natsorted(os.listdir(\"20_newsgroups/\" + folder_name))\n",
    " \n",
    "    # For every file.\n",
    "    for file_name in file_names:\n",
    " \n",
    "        # Read file contents.\n",
    "        stuff = read_file(\"20_newsgroups/\" + folder_name + \"/\" + file_name)\n",
    "         \n",
    "        # This is the list of words in order of the text.\n",
    "        # We need to preserve the order because we require positions.\n",
    "        # 'preprocessing' function does some basic punctuation removal,\n",
    "        # stopword removal etc.\n",
    "        final_token_list = preprocessing(stuff)\n",
    " \n",
    "        # For position and term in the tokens.\n",
    "        for pos, term in enumerate(final_token_list):\n",
    "             \n",
    "                    # First stem the term.\n",
    "                    term = stemmer.stem(term)\n",
    " \n",
    "                    # If term already exists in the positional index dictionary.\n",
    "                    if term in pos_index:\n",
    "                         \n",
    "                        # Increment total freq by 1.\n",
    "                        pos_index[term][0] = pos_index[term][0] + 1\n",
    "                         \n",
    "                        # Check if the term has existed in that DocID before.\n",
    "                        if fileno in pos_index[term][1]:\n",
    "                            pos_index[term][1][fileno].append(pos)\n",
    "                             \n",
    "                        else:\n",
    "                            pos_index[term][1][fileno] = [pos]\n",
    " \n",
    "                    # If term does not exist in the positional index dictionary\n",
    "                    # (first encounter).\n",
    "                    else:\n",
    "                         \n",
    "                        # Initialize the list.\n",
    "                        pos_index[term] = []\n",
    "                        # The total frequency is 1.\n",
    "                        pos_index[term].append(1)\n",
    "                        # The postings list is initially empty.\n",
    "                        pos_index[term].append({})     \n",
    "                        # Add doc ID to postings list.\n",
    "                        pos_index[term][1][fileno] = [pos]\n",
    " \n",
    "        # Map the file no. to the file name.\n",
    "        file_map[fileno] = \"20_newsgroups/\" + folder_name + \"/\" + file_name\n",
    " \n",
    "        # Increment the file no. counter for document ID mapping             \n",
    "        fileno += 1\n",
    " \n",
    "# Sample positional index to test the code.\n",
    "sample_pos_idx = pos_index[\"andrew\"]\n",
    "print(\"Positional Index\")\n",
    "print(sample_pos_idx)\n",
    " \n",
    "file_list = sample_pos_idx[1]\n",
    "print(\"Filename, [Positions]\")\n",
    "for fileno, positions in file_list.items():\n",
    "    print(file_map[fileno], positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd269d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
